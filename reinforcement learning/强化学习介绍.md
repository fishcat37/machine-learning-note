**智能体（Agent）** 在一个 **环境（Environment）** 中不断与环境交互，通过 **试错（trial-and-error）** 学习 **最优策略（Policy）**，以最大化长期累积的奖励（Reward）。

一个典型的强化学习系统通常由以下几个要素构成：

-   **Agent（智能体）**：做决策的学习者，例如机器人、游戏玩家。
    
-   **Environment（环境）**：智能体交互的对象，提供状态和奖励。
    
-   **State（状态）**：环境在某一时刻的信息描述。
    
-   **Action（动作）**：智能体在某一状态下能采取的操作。
    
-   **Reward（奖励）**：环境反馈给智能体的数值信号，用于衡量动作好坏。
    
-   **Policy（策略）**：从状态到动作的映射，指导智能体如何选择动作。
    
-   **Value Function（价值函数）**：评估某一状态（或状态-动作对）的“好坏”，即未来的长期回报预期。

**累计奖励**：$$
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

其中：

-   RRR 是奖励
    
-   $\gamma \in [0,1]$ 是折扣因子，用于平衡即时奖励和长期奖励

常见的强化学习方法大致可以分为：

1.  **基于值（Value-based）**
    
    -   典型算法：Q-learning、Deep Q-Network (DQN)
        
    -   学习状态-动作值函数 $Q(s, a)$，然后选择使 QQQ 最大的动作。
    
2.  **基于策略（Policy-based）**
    
    -   典型算法：REINFORCE、Actor-Critic
        
    -   直接学习策略函数 $\pi(a|s)$，即在某状态下采取某动作的概率。
    
3.  **Actor-Critic 混合方法**
    
    -   结合了值函数和策略函数，比如 A3C、PPO。