# 注意力改进

因为原始注意力需要很大的权重矩阵来计算q,k,v，所以导致很耗算力，特别是我们每次都使用整个注意力矩阵来计算，所以后续引入了很多方法来降低注意力计算开销

## 稀疏注意力



首先需要先回顾一下，对于一个token，经过$W_q$之后，会有一个q向量，在原始版注意力中，这个q会与所有token的k向量点乘再softmax获得每个token对这个token来说的权重，这个权重再与v向量相乘，就得到了每个token对于这个token来说的注意力，最后相加。

### Longformer

在longformer中，它使用一个滑动窗口控制每个token的上下文长度，从而使得计算量下降，因为他只与窗口内的token计算，但这样会引入一个问题，在一段文本中往往存在较为重要的内容，我们想要每个token都能和他计算注意力。所以就有了BigBird

### BigBird

BigBird中存在以下三种连接方式

#### 1\. **Local attention（局部窗口）**

-   每个 token 只和其左右固定窗口范围内的 token 进行注意力。
    
-   和 CNN 的卷积感受野类似。
    

#### 2\. **Global attention（全局token）**

-   选定一小部分重要的 token（如 `[CLS]` 或段首词）作为全局 token。
    
-   所有 token 都能与这些 global token 交互。
    
-   这些 token 是全连接 attention 的节点。
    

#### 3\. **Random attention（随机连接）**

-   每个 token 随机连接一些远处的 token，增强建模远距离依赖的能力。

### FlashAttention（现代大模型常用）

#### 背景问题

- Transformer 的注意力计算理论复杂度是 $O(n^2)$。
- 大部分实现内存访问效率低，显存占用大，导致训练长序列时显存瓶颈。

#### 核心思想

- **优化内存访问和计算顺序（IO-aware）**，用精心设计的 CUDA kernel，实现 **exact attention**。
- 通过将注意力计算划分成小块（tile），每次只计算并存储必要的子矩阵，极大减少显存峰值。
- 避免了传统注意力实现中存储整个 $n \times n$ 注意力矩阵。

#### 计算复杂度

- 理论依然是 $O(n^2)$，但在工程实现上极度高效，速度提升显著。
- 显存占用相比传统实现降低数倍。

#### 特点

- 计算的是**exact softmax attention**，没有近似。
- 速度上通常比 PyTorch 官方实现快 2-4 倍，且可支持更长序列。
- 支持 FP16 和混合精度训练。

#### 优点

- 几乎无损的精度。
- 大幅提升训练速度。
- 显存利用更高效，支持更长序列。
- 可替代所有标准 Transformer 注意力实现。

#### 缺点

- 依赖 GPU 硬件特性和 CUDA 实现。
- 需要相应硬件和软件环境支持。

现代大模型往往搭配**MQA**（Multi-Query Attention，多查询注意力）或者**GQA**（Grouped-Query Attention，分组查询注意力）一起使用，这两个都是作用于多头注意力的，原始多头注意力每个头都有自己的$W_q$ $W_k$ $W_v$权重矩阵，这意味着每个头的q,k,v都要独立计算，MQA让所有头共用一个$W_k$ $W_v$矩阵，这样k,v就只用计算一次，而每个头自己的都有$W_q$意味着输出不会一致，还是有多样性，而GQA是让多个头一组使用同样的$W_k$ $W_v$，这样能减少性能损失，又能一定的降低计算消耗。

## 线性注意力

将原本的注意力公式

$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d}}\right) V$

换成

$\text{Attention}(Q, K, V) \approx \phi(Q) \left(\phi(K)^T V\right)$