# 按结构和层级
## 自注意力
在编码器和掩码器中都存在,他关注自身,在掩码器中,他先出现,关注已完成部分,对于未预测部份,因为有mask,不会有注意力

## 交叉注意力
在transformer中,decoder会用到encoder最后输出的注意力过的token向量矩阵,在交叉注意力中,q生成的矩阵是掩码器自身的未mask的部分的,kv都是以encoder的输出生成的

# 按特性
## 掩码注意力
在计算q和k矩阵乘法得到q*k后对这个矩阵mask,将未来部份记为负无穷,让他的softmax为0,当前预测的token不会掩码

# 按种类
## 多头注意力
对比单头注意力,他直接减小了w,导致生成的去q,k,v矩阵变小,最后将几个头生成的矩阵在特征维度拼接,再作用于token向量矩阵

这样做的优势

1. 减少计算压力
2. 多个头,多套w,能学到更加灵活更加多样的信息

