# mamba

## RNN和Transformer的区别

| 模型        | 训练                           | 推理                           |
| ----------- | ------------------------------ | ------------------------------ |
| RNN         | 因果，时间复杂度随长度线性增长 | 因果，时间复杂度随长度线性增长 |
| Transformer | 非因果，与长度无关，可并行     | 因果，时间复杂度随长度平方增长 |

可以看到，Transformer对于RNN的优势就在于，它的训练是非因果的，故而可以进行高效的并行计算，充分利用GPU的多算子优势，故而它具有着一个巨大的优势，那就是能更好的训练出一个优秀的模型，但同时由于其推理是因果的，并且依赖于注意力，其时间复杂度是$n^2$的，所以其推理成本是巨大的，而因为传统的RNN类结构有着优秀的线性复杂度的推理，所以如果我们能改进RNN的训练使其变得可并行，就能得到一个在训练和推理上都具有巨大优势的模型，由这个思路衍生出了线性注意力和mamba等方法和模型

## 推导

这里使用两个公式表示RNN的隐藏状态和输出，其中f_a表示的是处理上一时刻隐藏状态的函数，f_b表示处理这一时刻输入的函数
$$
H_t=f_{A,t}(H_{t-1})+f_{B,t}(\mathbf{x_t})\\
\mathbf{y_t}=f_{C,t}(H_t)
$$
其中$H_t$表示t处的隐藏状态，$\mathbf{x_t}$表示t处的输入，$\mathbf{y_t}$表示t处的输出

> [!warning]
>
> 这里不要把这三个函数对应成了LSTM的遗忘门、输入门和输出门

在此给出从一开始的推导式
$$
\begin{cases}
H_1=f_{A,1}(H_{0})+f_{B,1}(\mathbf{x_1})\\
H_2=f_{A,2}(H_{1})+f_{B,2}(\mathbf{x_2})\\

\end{cases}
$$
以此类推将上一式带入下一式，发现会有很长的连续嵌套$f_{A,t}$，这就是导致其无法并行的问题所在，所以我们将遗忘门直接砍掉，直接将$H_{t-1}$作为输入，这样的话嵌套就解决了，我们再次推导发现嵌套没有了，但这样只能优化无法实现真正并行，因为输入门函数中还是依赖上一时刻的隐藏状态，所以还需要进行更改，我们将$f_{B,t}$的结果表示为$D_t$，将$\mathbf{y_t}=f_{C,t}(H_t)$变为$\mathbf{y_t}=f_{C,t}(H_t)=H_t\mathbf{q_t}$其中$\mathbf{q_t}=W_c\mathbf{x_t}$，所以
$$
H_t=H_0+D_1+D_2+......+D_t
$$
如果假设$H_0$直接为0，则变为：
$$
H_t=D_1+D_2+......+D_t
$$
则此时$y_t$可表示为：
$$
y_t=H_tq_t=D_1q_t+D_2q_t+......+D_tq_t
$$
此时可以改动一下，将D_t表示为$\mathbf{v_t}\mathbf{k_t}^\top$，其中$\mathbf{v_t}=W_v\mathbf{x_t}$$\mathbf{k_t}=W_k\mathbf{x_t}$，我们就能将输出化成形似注意力的形式，但需要注意的是D_t的计算中是需要一些非线性的映射的，所以实际使用中需要用一些核函数来取代这里的非线性映射函数如softmax，这样才能真正并行。通过对这个式子的各种改进来控制隐藏状态的取舍，引申出了很多模型，就比如mamba 