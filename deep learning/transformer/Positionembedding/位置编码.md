# 位置编码

## 绝对位置编码

### 可学习编码（bert）

在Bert中会有这样一个矩阵P

$\mathbf{P} \in \mathbb{R}^{L_{\max} \times d}$

$L_{\max}$是指的最大序列长度

$d$是指的Bert模型将token映射到的维度，例如768

所以我们可以看到这里的矩阵形状和token矩阵一致，所以对于词嵌入和位置嵌入，在bert中的做法是直接相加

$\mathbf{h}_i = \mathbf{w}_i + \mathbf{p}_i$

### 正余弦位置编码（transformer）

当序列长度是 $L$（最大长度），每个 token 向量维度是 $d$时

对于$pos \in [0, L-1]$位置上的token向量，他的第$i \in [0, d-1]$维（每个向量总维度为$d$）的位置编码为

对于偶数位置： $PE(pos, 2i) = \sin\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)$

对于奇数位置：$PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{\frac{2i}{d}}}\right)$

位置编码主要是为了体现句子中token的相对位置和绝对位置，其中这一个位置编码直接能体现绝对位置，而其相对位置体现在他的二阶矩中

## 相对位置编码

基本自注意力公式：$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V$

使用相对位置编码后的自注意力公式：$\text{softmax}(QK^\top + QR^\top)V$

$\text{score}_{i,j} = \underbrace{\mathbf{q}_i^\top \mathbf{k}_j}_{\text{内容相关性}} + \underbrace{\mathbf{q}_i^\top \mathbf{r}_{i-j}}_{\text{相对位置信息}} + \text{bias terms}$

这里的bias是两个偏置向量与$k_j$和$r_{i-j}$相乘后相加得到的，是为了对全局信息进行补偿

$\alpha_{i,j} = \text{softmax}_j\left( \frac{\mathbf{q}_i^\top \mathbf{k}_j + \mathbf{q}_i^\top \mathbf{r}_{i-j}}{\sqrt{d_k}} \right)$

$\mathbf{o}_i = \sum_{j=1}^{n} \alpha_{i,j} \mathbf{v}_j$

## 位置编码的使用

### 直接加到token embedded

在Bert中就是这么用的，这能带来信息的聚合，是特征聚合的一种方式，而这里不使用拼接的原因有几点

1. 会导致特征维度变化
2. 会导致参数量上升
3. 会导致模型能单独看到多种类别的嵌入表示，容易导致过于提高某类的重要性，使得过拟合

### 分开处理

因为在注意力计算中可以进行如下变换

$X' = X + PE$

$QK^\top = (XW^Q + PEW^Q)(XW^K + PEW^K)^\top$

$QK^\top = XW^QW^{K\top}X^\top + XW^QW^{K\top}PE^\top + PEW^QW^{K\top}X^\top + PEW^QW^{K\top}PE^\top$

我们可以发现第一项和第四项都只包含单一的token embedded和position embedded，而第二项和第三项就是他们的交叉，通过对他们进行相关性分析，发现无论是$X和PE^\top$还是$P和EX^\top$他们的相关性都不强，所以他们的交叉不太行，所以直接去掉中间两项，将位置编码和token编码分开计算，发现不仅预训练损失收敛的更快，下游任务的表现也更好了