# 预训练任务

## **Masked Language Model (MLM)**

- 随机遮盖输入序列中 **15%** 的 Token（如单词或子词）依据tokenizer的分词
- 遮盖策略采用动态混合方式：

  - **80%** 替换为 `[MASK]` 标记（如 "The cat sits on the `[MASK]`"）；
  - **10%** 替换为随机词（如 "The cat sits on an apple"）；
  - **10%** 保持原词不变（如 "The cat sits on the mat"）；

  在roberta和macbert中，将这一随机过程变为每个epoch都做一次，而原始bert是在预处理的时候做，后面就固定了，同时macbert认为使用mask替换会导致预训练与微调脱轨，因为微调时不会出现mask符号，所以他改成使用相似词替换，这里的相似词是用余弦相似度等方法找的

## **Next Sentence Prediction (NSP)**

**目标**：训练模型理解句子间的逻辑关系（如连贯性、因果性）。
 **机制**：

- 输入句子对$(A,B)$

  其中：

  - **50%** 的 `B` 是 `A` 的真实下一句（正例，标签为 `IsNext`）；
  - **50%** 的 `B` 是随机抽取的句子（负例，标签为 `NotNext`）；

## **句子顺序预测（Sentence Order Prediction, SOP）**

**SOP任务设计**：

- 正例：连续的两个句子（A→B）
- 负例：交换两个句子的顺序（B→A）
- 模型通过 `[CLS]`标记预测句子顺序是否正确

macbert使用SOP取代NSP更能获取句子间的逻辑关系

## **N-gram掩码（N-gram Masking）**：

 按不同长度的连续单元进行掩码，比例分配如下：

- 1-gram（单字）：40%
- 2-gram（双字短语）：30%
- 3-gram：20%
- 4-gram：10%

在macbert中有这么用，并且还有为中文优化的全词掩码（一次遮掩一整个词，比如：语言），提升了中文实体能力

## **全词掩码（Whole Word Masking, WWM）**

对多子词组成的完整词语（如英文"playing"拆分为"play"+"ing"）进行整体掩码，而非仅掩码部分子词，有着更优秀的实体能力
