# sigmoid

![](https://cdn.nlark.com/yuque/0/2025/png/54671003/1752151335982-7b830f8f-bcca-47c2-a427-66265025af2d.png)

![](https://cdn.nlark.com/yuque/0/2025/png/54671003/1752149471807-f8565c4b-22fc-4fec-a583-ad91252d0705.png)

缺点:没有负值,过大或过小容易梯度消失

# TanH

![](https://cdn.nlark.com/yuque/0/2025/png/54671003/1752151388638-85ff4ccc-1f33-4227-a9c2-661d7c8fdedd.png)

![](https://cdn.nlark.com/yuque/0/2025/png/54671003/1752149638943-05104ba0-62f3-4db7-8445-0a87e7af6a38.png)

优点:有负值

缺点:容易梯度消失

# ReLU

![](https://cdn.nlark.com/yuque/0/2025/png/54671003/1752151438578-61faf53f-cfb1-4752-b558-9c87311b443c.png)

![](https://cdn.nlark.com/yuque/0/2025/png/54671003/1752149828437-71082472-025b-489a-b079-d45279c439fc.png)

优点:不易梯度消失

缺点:会出现dead ReLU,无负值

# Leaky ReLU

## 表达式

![](https://cdn.nlark.com/yuque/0/2025/png/54671003/1752150128923-c8499c90-cca3-425b-8881-d6bd562e08d5.png)

$ f(n)=
\begin{cases}
x, &\text{if x>=0}\\
0.01x,&\text{if x<0}
\end{cases} $

a=0.01,可以取别的值,实际是希腊字母,a有两种取值方式,1使用均值为0,标准差为1的高斯分布随机取值,2把a作为参数学习

![](https://cdn.nlark.com/yuque/0/2025/png/54671003/1752150072369-2ed1e905-8690-498e-8f94-33998e2b2925.png)

# ELU

## 表达式

![](https://cdn.nlark.com/yuque/0/2025/png/54671003/1752150700081-1cf7fb60-21bf-4178-974b-4e5e0c6d0492.png)

这里的0.01是可调节参数,希腊字母a

![](https://cdn.nlark.com/yuque/0/2025/png/54671003/1752150670442-8aab964b-2fb4-4212-9009-67527d246479.png)

# swish

![](https://cdn.nlark.com/yuque/0/2025/png/54671003/1752150945719-3070a340-2d39-447e-8cae-e04237890bfc.png)

这里的b实际是希腊字母,GeLU使用的**标准正态分布的累积分布函数**，而它的近似实现中有一种就是使用sigmoid近似，其实就是b取**1.702**，还有一种是使用tanh近似$GeLU(x) ≈ 0.5x \left(1 + \tanh\left[\sqrt{\frac{2}{\pi}} (x + 0.044715x^3)\right]\right) $

![](https://cdn.nlark.com/yuque/0/2025/png/54671003/1752150921673-9485c34b-23a4-4c8e-889c-4ed2b6ebb253.png)

优点:效果最好,收敛最快

缺点:要计算指数,没有ReLU快

是现代大模型常用激活函数

# 评价指标

1. 既有负值又有正值
2. 像人体神经元一样,有一个激活阈值,低于他为0,即不激活,高于他会有不同的值,表示激活程度,类似于ReLU这种及他的改进型
3. 不易出现像sigmoid一样的梯度消失
4. 不易出现像ReLU一样的dead
5. 计算较快
