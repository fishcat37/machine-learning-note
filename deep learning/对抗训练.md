# 对抗训练

## 为什么会有对抗训练

这起源于一个思想,当我们对一个事物进行描述时,无论这种表示是否贴切,他一定是指向这个事物的,例如对于一张猫的图片来说,我们知道它是猫,但当我们对图片进行裁剪,缩放,遮掩,模糊后,我们可能不太能认出这是猫,但他实际上仍是猫.所以为了让模型能在这种情况下认出他是猫,所以我们认为构建这种较为模糊地表示加入到模型的训练中,是他具备辨别的能力,这种生成困难表示的方式就是对样本进行**数据扰动**

## 如何进行的数据扰动

### 1. FGM

在FGM中，数据扰动是让样本的每个特征**沿梯度方向移动**，使得模型损失最大程度增长，让模型更能学到知识，至于这里的梯度是如何来的，如果你写过FGM的代码，你会发现在一个batch中调用了两次model和loss，第一次正是用来计算梯度帮助第二次生成对抗样本的
$$
x_{adv} = x + ε * sign(∇_x L(x, y))
$$

```py
output=model(**batch)
loss=output.loss
loss.backward()
fgm.attack()
output2=model(**batch)
loss2=output2.loss
loss2.backward()
fgm.restore()
```



### 2. PGD

在PGD中，他不只会生成一次对抗样本，他会生成多次对抗样本并迭代优化

第一个样本
$$
x_{1} = x_{0} + clip(x_{t+1}, x-ε, x+ε)
$$
后续样本
$$
x_{t+1} = x_t + α * sign(∇_x L(x_t, y))\\
x_{t+1} = clip(x_{t+1}, x-ε, x+ε)
$$
PGD多次生成对抗样本并优化的优势

1. 攻击能力更强
2. 搜索更充分
3. 克服局部最优解

缺点显而易见，太耗时间了，所以对于PGD而言，对抗次数是一个重要的超参数，需要平衡性能与效果

