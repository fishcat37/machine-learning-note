在深度学习中,我们需要通过损失反向更新模型参数,而这个反向更新参数的过程就由优化器控制,传统优化器只负责简单的更新参数,他不包含动量和动态学习率,代表就是SGD

# 动量
## 标准动量
计算当前梯度$ g_t = ∇_θ L(θ_{t-1}) $

计算动量$ v_t = β * v_{t-1} + (1 - β) * g_t $

更新参数$ θ_t = θ_{t-1} - α * v_t $

## Nesterov Accelerated Gradient (NAG)
$ v_t = γ * v_{t-1} + η * ∇J(θ_{t-1} - γ * v_{t-1}) $

$ θ_t = θ_{t-1} - v_t $

可以看到,动量是动态变化的,因为模型更新,所以他不断更新自身,同时为了保留历史信息,他选择了这种动量的形式

动量就如现实中物理中的动量,它揭示了模型更新的整体方向,通过动量,我们能做到以下:

1. 加快收敛
2. 抑制震荡
3. 跳出局部最小值

# 动态学习率
在传统的学习率下,会有这样的问题: 适合大家的,不一定适合个别,适合大多数的,不一定适合少数,例如,对于大多数参数来说,增大n最优,但对于有些参数,增大n/2或者2*n最优,所以传统的方式每次更新都不是很理想,而引入动态学习率就可以弥补这个,在使用动态学习率的优化器中,相当于为每个参数单度设置了一个学习率缩放因子,通过这样,能在保持全局学习率的同时使得每个参数的学习率更适合他,能避免下降中的震荡,以及下降过慢的问题,但要注意的是,他只能帮助更快更稳的接近局部最优,但他不能帮助跳出当前局部最优寻找更好的局部最优,对于历史梯度大的参数,缩放因子应该小,防止震荡,对于历史梯度小的参数,缩放因子应该大,加快下降

# batch_size
## 小批量更新
在传统机器学习中,我们通常使用全量数据更新参数,训练多轮来达到最优,但这通常不适合深度学习,有多点原因

1. 深度学习使用的数据量往往比传统机器学习大,且大得多,这不仅仅是因为数据集的加大,还因为深度学习主要处理领域的特点,无论图片,音频,文字,他们的数据量都会很大,直接使用全量数据会使得gpu内存不够,并且深度学习模型的模型参数量也大
2. 小批量带来一定噪声,在深度学习中,噪声往往不是一个坏词,由于深度学习模型的性能往往更强,所以他们也更容易过拟合,所以往往引入噪声不是一件坏事,而且对比单样本,他的噪声不至于过大
3. 由于单批次多个样本能并行,发挥gpu优势
4. 节省内存
5. 学习的快,因为参数更新次数多

## 单样本更新
优点:

1. 计算效率高,单次只计算一个
2. 噪声大,增强模型泛化能力
3. 非常节省内存

缺点:

1. 收敛不稳定
2. 全局收敛慢,因为过于震荡

## 全量更新
优点:

1. 模型收敛平稳,因为使用的是全部数据
2. 梯度准确

缺点:

1. 计算成本大
2. 内存占用高
3. 参数更新慢
4. 泛化能力差,噪声少







